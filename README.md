# ðŸ“Š Sanshrit's Data Science & Machine Learning Portfolio

Welcome to my portfolio! I'm Sanshrit Singhai â€” a Data Scientist with experience building end-to-end ML and Generative AI solutions across logistics, finance, and research domains.

This repository showcases a curated collection of my projects spanning supervised learning, unsupervised learning, optimization, generative AI, and time series forecasting.

---

## ðŸ” Projects Overview

| Project | Description | Key Technologies |
|--------|-------------|------------------|
| [ðŸ“‰ Credit Card Fraud Detection](https://github.com/ssinghai6/Fraud_transactions) | Built an ML pipeline to classify fraudulent transactions using XGBoost and Random Forest; handled imbalanced data using SMOTE and custom fraud pattern features. | XGBoost, Random Forest, SMOTE, pandas, matplotlib |
| [ðŸ§  RAG-based Document Chatbot](https://github.com/ssinghai6/DocBot) | Built a Retrieval-Augmented Generation (RAG) chatbot using LangChain, LLaMA 3.2, and FAISS for intelligent document querying. | LangChain, LLaMA 3.2, FAISS, Streamlit |
| [ðŸš› Logistics Optimization Solver] | Developed integer programming models to optimize fleet-wide truck assignment based on profit, integrated with OR-Tools and Gurobi. | Python, OR-Tools, Gurobi, optimization |
| [ðŸ“ˆ Chicago Crime Prediction](https://github.com/ssinghai6/CSE6242-DVA-Team007) | Analyzed 7M+ crime records and built predictive models to forecast crime likelihood across ZIP codes. | PySpark, AWS, Tableau, Flask, XGBoost |
| [ðŸ”¬ ML Stress Field Predictor](https://github.com/ssinghai6/Intelligent-Tunneling-Machine-Learning-based-Prediction-) [ðŸ”—](https://ce.gatech.edu/news/researchers-receive-17-million-grant-build-robot-subsurface-soil-exploration) | Replaced FEM simulations with deep learning models (Autoencoders, ANN) to predict stress distribution in geotechnical domains; part of a $17M NSF-funded initiative. | TensorFlow, Autoencoders, ANN |

---

## ðŸ§  Skills Highlighted

- **Languages:** Python, SQL, Julia, C++, JavaScript  
- **ML/DL Tools:** Scikit-learn, TensorFlow, PyTorch, XGBoost, LangChain  
- **Cloud & Infra:** Azure, AWS, Docker, PySpark, Event Hubs  
- **Data Viz:** Tableau, Power BI, matplotlib  
- **Optimization:** OR-Tools, Gurobi, Integer Programming  

---

## ðŸ“« Connect With Me

- [LinkedIn](https://www.linkedin.com/in/singhai-sanshrit/)  
- [Website](https://sanshrit-singhai.vercel.app/)  
- [Email](mailto:singhai.sanshrit@live.com)

---
## âœ¨ Key Features

- **Interactive Experience Timeline**: Collapsible role details with "at-a-glance" summaries and tech tags.
- **Dynamic Blog System**: Automated AI news curation with manual "view archive" controls.
- **Optimized Performance**: Glassmorphism UI with fast load times and clean, responsive layout.

---

## ðŸ¤– AI News Agent

This portfolio includes an **automated AI News Agent** built with LangGraph that curates weekly AI/ML news for the blog section. It features email-based approval - just reply "APPROVE" to publish!

### âœ¨ Features

- ðŸ”„ **Automated Weekly Updates**: Runs every Monday at 9 AM UTC
- ðŸ§  **LangGraph + Groq**: Uses Llama 3.3 (free) for intelligent summarization
- ðŸ“§ **Email Approval**: Reply to GitHub notification email with "APPROVE"
- ðŸ“ **Rich Content**: Generates well-structured articles with sections, bullet points, and sources
- ðŸ”— **Source Attribution**: All articles include linked sources

### ðŸ“§ Email Approval Flow

```
1. Agent runs (Monday 9 AM) â†’ Generates article
                â†“
2. Creates GitHub Issue â†’ GitHub emails you
                â†“
3. You reply "APPROVE" to the email
                â†“
4. GitHub posts your reply as a comment
                â†“
5. Workflow detects "APPROVE" â†’ Publishes to blog!
```

### ðŸ—ï¸ Architecture

```
LangGraph State Machine:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Search Node â”‚ â†’ â”‚ Summarize    â”‚ â†’ â”‚ Create Issue   â”‚
â”‚ (Serper)    â”‚    â”‚ Node (Groq)  â”‚    â”‚ Node (GitHub)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“                   â†“                    â†“
   News API          Llama 3.3           GitHub API
   (optional)        (free tier)         (Issues)
```

### ðŸš€ Setup Instructions

#### 1. Get API Keys

| Service | URL | Required? |
|---------|-----|-----------|
| **Groq** | https://console.groq.com | âœ… Yes (free) |
| **Serper** | https://serper.dev | âŒ Optional |

#### 2. Add GitHub Secrets

1. Go to your repo â†’ **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Add `GROQ_API_KEY` with your Groq key
3. (Optional) Add `SERPER_API_KEY` for real news search

#### 3. Test the Agent

**Locally:**
```bash
# Create .env file in agent/ folder
echo "GROQ_API_KEY=your_key_here" > agent/.env

# Run the agent
cd portfolio
pip install -r agent/requirements.txt
python3 agent/news_agent.py
```

**On GitHub:**
1. Go to **Actions** tab
2. Click **"Weekly AI News Agent"**
3. Click **"Run workflow"**

### ðŸ“ File Structure

```
agent/
â”œâ”€â”€ news_agent.py      # LangGraph agent (search â†’ summarize â†’ create issue)
â”œâ”€â”€ publish_agent.py   # Publishes post when approved
â”œâ”€â”€ requirements.txt   # Python dependencies
â””â”€â”€ .env              # API keys (gitignored)

.github/workflows/
â”œâ”€â”€ weekly_agent.yml   # Runs agent every Monday
â””â”€â”€ approve_post.yml   # Triggered by "APPROVE" comment
```

### ðŸ”§ Customization

- **Change schedule**: Edit cron in `weekly_agent.yml`
- **Change model**: Update `model_name` in `news_agent.py`
- **Add news sources**: Modify `search_news_node()` function

### ðŸ§ª Test Cases

#### Agent Tests (Local)

| Test Case | Command | Expected Result |
|-----------|---------|-----------------|
| **1. Dry run (no API keys)** | `python3 agent/news_agent.py` | Uses mock data, generates sample post |
| **2. With Groq API** | Set `GROQ_API_KEY` in `.env`, run agent | Generates real AI-written content |
| **3. With Serper API** | Set `SERPER_API_KEY` in `.env`, run agent | Fetches real news articles |
| **4. Import check** | `python3 -c "from langgraph.graph import StateGraph"` | No errors |

#### Workflow Tests (GitHub)

| Test Case | How to Test | Expected Result |
|-----------|-------------|-----------------|
| **5. Manual trigger** | Actions â†’ Weekly AI News Agent â†’ Run workflow | Issue created with AI content |
| **6. Email notification** | After Issue creation | Email received at GitHub email |
| **7. Approve via comment** | Comment "APPROVE" on Issue | Post added to `blogPosts.json` |
| **8. Approve via email** | Reply "APPROVE" to GitHub email | Same as above |

#### UI Tests (Frontend)

| Test Case | How to Test | Expected Result |
|-----------|-------------|-----------------|
| **9. Blog section loads** | Navigate to `/#blog` | Shows 2 most recent posts |
| **10. View All modal** | Click "AI/ML Insights" | Modal shows all posts |
| **11. Read More modal** | Click "Read more" on a post | Detailed article with sections |
| **12. Source links** | Click source in Read More modal | Opens source URL in new tab |
| **13. Mobile responsive** | Resize browser to mobile | Cards stack vertically |

#### Edge Cases

| Test Case | Scenario | Expected Result |
|-----------|----------|-----------------|
| **14. No API keys** | Remove all API keys | Falls back to mock data |
| **15. Serper returns 0** | Invalid Serper key | Falls back to mock news |
| **16. JSON parse error** | LLM returns malformed JSON | Falls back to simplified post |
| **17. Empty blog posts** | Delete all from `blogPosts.json` | Shows empty state gracefully |

#### Running All Tests

```bash
# 1. Test Python imports
python3 -c "from langgraph.graph import StateGraph; from langchain_groq import ChatGroq; print('âœ… All imports work')"

# 2. Test agent dry run
python3 agent/news_agent.py

# 3. Test React app
npm start
# Visit http://localhost:3000/#blog
```

---

> ðŸ’¡ *This portfolio reflects both academic and industry work â€” combining research depth with production-ready data science.*  